---
title: "PS4 Nasser & Daniel"
format: 
  pdf:
    keep-tex: true
    include-in-header: 
       text: |
         \usepackage{fvextra}
         \DefineVerbatimEnvironment{Highlighting}{Verbatim}{breaklines,commandchars=\\\{\}}
include-before-body:
  text: |
    \RecustomVerbatimEnvironment{verbatim}{Verbatim}{
      showspaces = false,
      showtabs = false,
      breaksymbolleft={},
      breaklines
    }
---

**PS4:** Due Sat Nov 2 at 5:00PM Central. Worth 100 points. 
We use (`*`) to indicate a problem that we think might be time consuming. 
    
## Style Points (10 pts) 
Please refer to the minilesson on code style
**[here](https://uchicago.zoom.us/rec/share/pG_wQ-pHTQrJTmqNn4rcrw5V194M2H2s-2jdy8oVhWHkd_yZt9o162IWurpA-fxU.BIQlSgZLRYctvzp-)**.

## Submission Steps (10 pts)
1. This problem set is a paired problem set.
2. Play paper, scissors, rock to determine who goes first. Call that person *Partner 1*.
    - Partner 1 (Nasser Alshaya and alshaya):
    - Partner 2 (name and cnet ID):
3. Partner 1 will accept the `ps4` and then share the link it creates with their partner. You can only share it with one partner so you will not be able to change it after your partner has accepted. 
4. "This submission is our work alone and complies with the 30538 integrity policy." Add your initials to indicate your agreement: `NA`, \*\*\_\_\*\*
5. "I have uploaded the names of anyone else other than my partner and I worked with on the problem set **[here](https://docs.google.com/forms/d/185usrCREQaUbvAXpWhChkjghdGgmAZXA3lPWpXLLsts/edit)**"  (1 point)
6. Late coins used this pset: `1` Late coins left after submission: `3`
7. Knit your `ps4.qmd` to an PDF file to make `ps4.pdf`, 
    * The PDF should not be more than 25 pages. Use `head()` and re-size figures when appropriate. 
8. (Partner 1): push  `ps4.qmd` and `ps4.pdf` to your github repo.
9. (Partner 1): submit `ps4.pdf` via Gradescope. Add your partner on Gradescope.
10. (Partner 1): tag your submission in Gradescope

**Important:** Repositories are for tracking code. **Do not commit the data or shapefiles to your repo.** The best way to do this is with `.gitignore`, which we have covered in class. If you do accidentally commit the data, Github has a [guide](https://docs.github.com/en/repositories/working-with-files/managing-large-files/about-large-files-on-github#removing-files-from-a-repositorys-history). The best course of action depends on whether you have pushed yet. This also means that both partners will have to download the initial raw data and any data cleaning code will need to be re-run on both partners' computers. 

## Download and explore the Provider of Services (POS) file (10 pts)

1. I pulled the columns related to: Provider Subtype, Provider Type, Number of Times of Changing Ownership, Effective Date of Most Recent Ownership Change, City Name, Facility Name, CMS Certification Number, Termination Status, Date the provider was Terminated, and Zip Code.

```{python}
import pandas as pd
import os
base_path = r"c:/Users/danie/Documents/GitHub/problem-set-4-nasser-daniel"
path_data = os.path.join(base_path,"pos2016.csv")
df = pd.read_csv(path_data)
df.columns
```

2. 
    a. There are `7245` hosptials reported in this data as short-term hospitals in `2016` Q4. The number is too large to be true, we would assume that most of the hospitals of this type are no longer in service.
    b. Accordin to **[2016 CMS Statistics](https://www.cms.gov/Research-Statistics-Data-and-Systems/Statistics-Trends-and-Reports/CMS-Statistics-Reference-Booklet/Downloads/2016_CMS_Stats.pdf)**, the number of short-term hospitals as of December 2015 is `3436`, the number is way smaller than what we found in (a), even if the difference is only one year. 
```{python}
df_2016 = df[
    (df["PRVDR_CTGRY_SBTYP_CD"] == 1) & 
    (df["PRVDR_CTGRY_CD"] == 1)]
len(df_2016)
```

3. The number of short-term hospitals per is plotted below, the number of hospitals increases every year, starting from 7245 in 2016 reaching to 7303 in 2019. 
```{python}
path_data = os.path.join(base_path,
 "pos2017.csv")
df_2017 = pd.read_csv(path_data)
 
path_data = os.path.join(base_path,
 "pos2018.csv")
df_2018 = pd.read_csv(path_data, encoding_errors = "ignore")

path_data = os.path.join(base_path,
 "pos2019.csv")
df_2019 = pd.read_csv(path_data, encoding_errors = "ignore")
```


```{python}
df_2017 = df_2017[
    (df_2017["PRVDR_CTGRY_SBTYP_CD"] == 1) & 
    (df_2017["PRVDR_CTGRY_CD"] == 1)]

df_2018 = df_2018[
    (df_2018["PRVDR_CTGRY_SBTYP_CD"] == 1) & 
    (df_2018["PRVDR_CTGRY_CD"] == 1)]

df_2019 = df_2019[
    (df_2019["PRVDR_CTGRY_SBTYP_CD"] == 1) & 
    (df_2019["PRVDR_CTGRY_CD"] == 1)]

# Add year column for each df for plot:
df_2016["year"] = 2016
df_2017["year"] = 2017
df_2018["year"] = 2018
df_2019["year"] = 2019
df_pos = pd.concat([df_2016,df_2017,df_2018,df_2019],
 ignore_index = True)
```

```{python}
import altair as alt
import warnings 
warnings.filterwarnings('ignore')
alt.renderers.enable("png")
alt.data_transformers.disable_max_rows()
bars = alt.Chart(df_pos).mark_bar().encode(
    alt.X("year:N", title = "Year"),
    alt.Y("count()",title = None, axis = None)
).properties(
    title = "Number of Short-term Hospitals per Year",
    width = 350, height = 200)
text = bars.mark_text(
    align="center",
    baseline="bottom",
    dy=-5  
).encode(
    text=alt.Text("count():Q")  
)

bars + text
```

4. 
    a. Upon aggregating the data based on the CMS certificate of each hospital, it seems that the numbers are identical to what previously found in (3)

```{python}
bars = alt.Chart(df_pos).transform_aggregate(
    unique_hospitals="count(PRVDR_NUM)",groupby=["year"]
    ).mark_bar().encode(
        x=alt.X("year:N", title="Year"),
    y=alt.Y("unique_hospitals:Q", axis = None, title = None)
).properties(
    title="Count of Unique Hospitals per Year",
    width=350,
    height=200
)
text = bars.mark_text(
    align="center",
    baseline="bottom",
    dy=-5  
).encode(
    text=alt.Text("unique_hospitals:Q")  
)

bars + text
```

    b. Upon comparing the two plots, the number of hospitals per year is identical to the number of unique hospitals based on the CMS certificate, this tells us that the data is strucutured in a way that prevents duplicate entries of hospitals per year, and the records are well strucutured to represent the number of hospitals accurately.

  
## Identify hospital closures in POS file (15 pts) (*)

1. There are `3403` hospitals that fit the definition of suspected closure.

```{python}
suspected_hospitals = []

for hospital in df_pos["PRVDR_NUM"].unique():
    for year in range(2016, 2020):
        df_hospital_year = df_pos[(df_pos["PRVDR_NUM"] == hospital) & 
        (df_pos["year"] == year)]
        if (df_hospital_year["PGM_TRMNTN_CD"].values != 0 &
            pd.isna(df_hospital_year["PGM_TRMNTN_CD"])).any():
            suspected_hospitals.append({
            "FAC_NAME": df_hospital_year["FAC_NAME"].iloc[0],
            "ZIP_CD": df_hospital_year["ZIP_CD"].iloc[0],
            "closure_year": year
        })
            break
```


```{python}
df_closures = pd.DataFrame(suspected_hospitals)
len(df_closures["ZIP_CD"].unique())
```

2. Facility names of suspected closure hospitals sorted by name for the first 10 results:

```{python}

df_closures.sort_values(by = "FAC_NAME",ascending=True)[["FAC_NAME", "closure_year"]].head(10)
```

3. 
    a. There are `388` hospitals that fit this definition of potentially being a merger/acquisition.

```{python}
Active_hospitals = df_pos[df_pos["PGM_TRMNTN_CD"] == 0].groupby(
    ["year","ZIP_CD"])["PGM_TRMNTN_CD"].size().reset_index(name = "Active_count")

Active_grouped = Active_hospitals.groupby(["ZIP_CD", "year"]).size().reset_index(name = "count")

zips_grouped = df_pos.groupby(["ZIP_CD", "year"])["PGM_TRMNTN_CD"].size().reset_index(name = "count")

Active_merged = Active_grouped.merge(zips_grouped, on = ["ZIP_CD","year"] , how = "inner")

# Difference between active zipcodes and all zipcodes

Active_merged = Active_merged.sort_values(["ZIP_CD", "year"])

Active_merged["prev_year_count_y"] = Active_merged.groupby("ZIP_CD")["count_y"].shift(1)

Active_merged["decreased"] = Active_merged["count_x"] < Active_merged["prev_year_count_y"]

decreased_zips = Active_merged[Active_merged["decreased"]]
```

```{python}

df_closures = df_closures[-df_closures["ZIP_CD"].isin(decreased_zips["ZIP_CD"])]
```


```{python}
len(df_closures["ZIP_CD"].unique())
```

    b. Upon correcting we have 2697 hospitals left

    c. Sorted list shown below:

```{python}
df_closures.sort_values(by = "FAC_NAME",ascending=True)[["FAC_NAME", "closure_year"]].head(10)
```

## Download Census zip code shapefile (10 pt) 

1. 
    a. There are five files zipped in this folder as the following:

    `dbf`: stores attribute information of spatial features, size 6.4 MB.
    `shp`: shape file which has has feature geometrics, size 837.5 MB.
    `prj`: describes the Coordinate Reference System (CRS), size 165 bytes.
    `shx`: contains positional index in .shp, size 265 KB.
    `xml`: metadata about the dataset, size 16 KB.

    b.     `dbf`:size 6.4 MB.
    `shp`: size 837.5 MB.
    `prj`: size 165 bytes.
    `shx`: size 265 KB.
    `xml`: size 16 KB.
    
2. There are 495 active hospitals in Texas for the year 2016.
```{python}
import geopandas as gpd 
```

```{python}
filepath = "c:/Users/danie/Documents/GitHub/problem-set-4-nasser-daniel"

path_shp = os.path.join(filepath,
 "gz_2010_us_860_00_500k.shp")

df_zip = gpd.read_file(path_shp)
```

```{python}
texas_zip_codes = df_zip[df_zip["ZCTA5"].str.startswith(
    ("75","76","77","78","79"))]
```

```{python}
# 2016 Hoispitals:
df_hospitals_2016 = df_2016
df_hospitals_2016 = df_pos[-df_pos["ZIP_CD"].isin(df_closures)]
```

```{python}
# Number of Hospitals in Texas:
texas_zips = texas_zip_codes["ZCTA5"].astype(int).tolist()
df_hospitals_2016["ZIP_CD"] = df_hospitals_2016["ZIP_CD"].astype(int)
grouped_hospitals_zip = df_hospitals_2016[df_hospitals_2016["ZIP_CD"].isin(texas_zips)]
len(grouped_hospitals_zip.groupby("ZIP_CD"))
```

```{python}
# create a list of filtered zips to compare with Texas zips in shp file

hospital_zips_2016 = df_hospitals_2016["ZIP_CD"].astype("str").to_list()

filtered_zips_2016 = [zip_code for zip_code in hospital_zips_2016 if zip_code.startswith(("75",
  "76", "77", "78", "79"))]
# Boolean column to flag hospitals
texas_zip_codes["Hospital_zips"] = texas_zip_codes["ZCTA5"].isin(
    filtered_zips_2016).astype(int)

import matplotlib.pyplot as plt
texas_zip_codes.plot(column = "Hospital_zips", linewidth=0.2)
plt.axis("off")
plt.show()
```


## Calculate zip code’s distance to the nearest hospital (20 pts) (*)

1. 

```{python}
filepath = "c:/Users/danie/Documents/GitHub/problem-set-4-nasser-daniel"

# entire united states
path_shp = os.path.join(filepath,
 "gz_2010_us_860_00_500k.shp")
df_geo = gpd.read_file(path_shp)

```

```{python}
zips_all_centroids = df_geo.centroid
zips_all_centroids.shape
```

The resulting dimensions of this dataframe are (33120, 0)

2. 

```{python}
# just texas
zips_texas_centroids = texas_zip_codes["geometry"].centroid
zips_texas_centroids.shape
```

there are 482 unique zipcodes texas

```{python}
# texas and surrounding area
texas_range = [i for i in range(75, 80)]
missouri_range = [i for i in (range(716, 730))]
oklahoma_range = [i for i in (range(73, 75))]
new_mexico_range = [i for i in (range(870, 885))]
louisiana_range = [i for i in (range(700, 716))]

texas_and_surrounding_states_zips = []
texas_and_surrounding_states_zips.append(texas_range)
texas_and_surrounding_states_zips.append(missouri_range)
texas_and_surrounding_states_zips.append(oklahoma_range)
texas_and_surrounding_states_zips.append(new_mexico_range)
texas_and_surrounding_states_zips.append(louisiana_range)

#unpacking the list of lists
texas_and_surrounding_states_zips_list = pd.Series([zip for sublist in texas_and_surrounding_states_zips for zip in sublist]).astype(str)

texas_and_surrounding_states_zips_tuple = []
for i in range(len(texas_and_surrounding_states_zips_list)):
    value = texas_and_surrounding_states_zips_list[i]
    texas_and_surrounding_states_zips_tuple.append(value)

texas_and_surrounding_states_zips_tuple = tuple(texas_and_surrounding_states_zips_tuple)

#identifying texas and surrounding zipcodes from geodataframe
hospital_zips_2016 = cleaned_pos_2016["ZIP_CD"].astype(int).astype(str).tolist()
filtered_zips_2016 = [zip_code for zip_code in hospital_zips_2016 if
 zip_code.startswith(texas_and_surrounding_states_zips_tuple)]

texas_and_surrounding_zip_codes = df_zip["ZCTA5"].isin(filtered_zips_2016)
texas_and_surrounding_zip_codes = df_zip[texas_and_surrounding_zip_codes]
texas_and_surrounding_zip_codes["Hospital_zips"] = texas_and_surrounding_zip_codes["ZCTA5"]

zips_texas_borderstates_centroids = texas_and_surrounding_zip_codes["geometry"].centroid
zips_texas_borderstates_centroids.plot()
zips_texas_borderstates_centroids.shape

```

There are 868 unique zip codes in the texas and surrounding states centroid variable. 

3. 

```{python}
active_hospitals_2016 = df_pos[df_pos["PGM_TRMNTN_CD"] == 0]["ZIP_CD"].astype(int).unique()
#take these zip codes and find them in the geo dataset
min_one_hospital = df_geo["ZCTA5"].astype(int).isin(active_hospitals_2016)
min_one_hospital = df_geo[min_one_hospital]
min_one_hospital["ZIP_CD"] = min_one_hospital["ZCTA5"]

zips_withhospital_centroids = gpd.sjoin(texas_and_surrounding_zip_codes, min_one_hospital, how = "inner").centroid

zips_withhospital_centroids.plot()
```

4. 

    a.

```{python}

distance_gdf = gpd.GeoDataFrame()
distance_gdf['nearest_hospital_distance'] = zips_withhospital_centroids.geometry.apply(
    lambda x: zips_texas_borderstates_centroids.head().distance(x).min())

distance_gdf
```

This shortened version took 16 seconds. The longer version could take multiple minutes.

```{python}

distance_gdf_full = gpd.GeoDataFrame()
distance_gdf_full['nearest_hospital_distance'] = zips_withhospital_centroids.geometry.apply(
    lambda x: zips_texas_borderstates_centroids.distance(x).min())

distance_gdf_full
```

    b. I fooled myself. This was much quicker than I expected. It seems that every zipcode has at least one active hospital in it in 2016. Therefore the computer did not have many intensive calculations to complete, relative to the first attempt at this.
5. 
    a.
    b.
    c.
    
## Effects of closures on access in Texas (15 pts)

1. The first 10 rows of the table for closures count per zip code is shown below.

```{python}
df_closures_texas = df_closures[df_closures["ZIP_CD"].isin(texas_zips)]
texas_closures = (
    df_closures_texas.groupby("ZIP_CD").size().reset_index(name = "closures_count"))

texas_closures.head()
```

2. There are 253 zipcodes that were directly affected by a closure in 2016 to 2019.

```{python}
len(texas_closures)
```

```{python}
texas_zip_codes.plot(column = "Hospital_zips", linewidth=0.2)
plt.axis("off")
plt.show()
```

```{python}
direct_zips = texas_closures["ZIP_CD"].astype(int).astype(str).to_list()
texas_direct = texas_zip_codes[texas_zip_codes["ZCTA5"].isin(direct_zips)]
```

```{python}
texas_zip_codes["direct"] = texas_zip_codes["ZCTA5"].isin(direct_zips)
texas_zip_codes.plot(column = "direct", linewidth=0.2)
plt.axis("off")
plt.show()
```

3. There 970 indirectly affected zip codes are there in Texas.

```{python}
# meters to miles 1 mi = 1.6093 Km:
texas_direct["buffer_10mi"]= texas_direct.buffer(16093)
spatial_joint = texas_zip_codes.sjoin(texas_direct, how = "left")

# indirectly affected zips:
filtered_spatial_joint = spatial_joint[
    spatial_joint["buffer_10mi"].notnull()]

direct_zips_set = set(direct_zips) 
indirect_zips_set = set(
    filtered_spatial_joint["ZCTA5_left"].values) - direct_zips_set

len(indirect_zips_set)
```

4. 

```{python}
spatial_joint["affect_type"] = ["directly_affected" if zip_code in
 direct_zips_set else "indirectly_affected" if zip_code in 
 indirect_zips_set else "not_affected" for zip_code in spatial_joint
 ["ZCTA5_left"]]
```

```{python}
spatial_joint.plot(column = "affect_type", linewidth=0.2, legend = True,legend_kwds={"loc": "lower left"})
plt.axis("off")

plt.show()
```

## Reflecting on the exercise (10 pts) 

1. I believe having a unique identifier for each hospital is the best way to keep track all the changes over time. It would be easier than relying on a zipcode, facility name, or a CMS certificate for the ease of analysis and tracking changes over time.