---
title: "PS4 Nasser & Daniel"
format: 
  pdf:
    keep-tex: true
    include-in-header: 
       text: |
         \usepackage{fvextra}
         \DefineVerbatimEnvironment{Highlighting}{Verbatim}{breaklines,commandchars=\\\{\}}
include-before-body:
  text: |
    \RecustomVerbatimEnvironment{verbatim}{Verbatim}{
      showspaces = false,
      showtabs = false,
      breaksymbolleft={},
      breaklines
    }
---

**PS4:** Due Sat Nov 2 at 5:00PM Central. Worth 100 points. 
We use (`*`) to indicate a problem that we think might be time consuming. 
    
## Style Points (10 pts) 
Please refer to the minilesson on code style
**[here](https://uchicago.zoom.us/rec/share/pG_wQ-pHTQrJTmqNn4rcrw5V194M2H2s-2jdy8oVhWHkd_yZt9o162IWurpA-fxU.BIQlSgZLRYctvzp-)**.

## Submission Steps (10 pts)
1. This problem set is a paired problem set.
2. Play paper, scissors, rock to determine who goes first. Call that person *Partner 1*.
    - Partner 1 (Nasser Alshaya and alshaya):
    - Partner 2 (name and cnet ID):
3. Partner 1 will accept the `ps4` and then share the link it creates with their partner. You can only share it with one partner so you will not be able to change it after your partner has accepted. 
4. "This submission is our work alone and complies with the 30538 integrity policy." Add your initials to indicate your agreement: \*\*\NA\*\* \*\*\_\_\*\*
5. "I have uploaded the names of anyone else other than my partner and I worked with on the problem set **[here](https://docs.google.com/forms/d/185usrCREQaUbvAXpWhChkjghdGgmAZXA3lPWpXLLsts/edit)**"  (1 point)
6. Late coins used this pset: \*\*\_\_\*\* Late coins left after submission: \*\*\_\_\*\*
7. Knit your `ps4.qmd` to an PDF file to make `ps4.pdf`, 
    * The PDF should not be more than 25 pages. Use `head()` and re-size figures when appropriate. 
8. (Partner 1): push  `ps4.qmd` and `ps4.pdf` to your github repo.
9. (Partner 1): submit `ps4.pdf` via Gradescope. Add your partner on Gradescope.
10. (Partner 1): tag your submission in Gradescope

**Important:** Repositories are for tracking code. **Do not commit the data or shapefiles to your repo.** The best way to do this is with `.gitignore`, which we have covered in class. If you do accidentally commit the data, Github has a [guide](https://docs.github.com/en/repositories/working-with-files/managing-large-files/about-large-files-on-github#removing-files-from-a-repositorys-history). The best course of action depends on whether you have pushed yet. This also means that both partners will have to download the initial raw data and any data cleaning code will need to be re-run on both partners' computers. 

## Download and explore the Provider of Services (POS) file (10 pts)

1. I pulled the columns related to: Provider Subtype, Provider Type, Number of Times of Changing Ownership, Effective Date of Most Recent Ownership Change, City Name, Facility Name, CMS Certification Number, Termination Status, Date the provider was Terminated, and Zip Code.

```{python}
import pandas as pd
import os
base_path = r"c:/Users/danie/Documents/GitHub/problem-set-4-nasser-daniel"
path_data = os.path.join(base_path,"pos2016.csv")
df = pd.read_csv(path_data)
df.columns
```
```{python}
df_2016 = df[
    (df["PRVDR_CTGRY_SBTYP_CD"] == 1) & 
    (df["PRVDR_CTGRY_CD"] == 1)]
len(df_2016)
```

3. The number of short-term hospitals per is plotted below, the number of hospitals increases every year, starting from 7245 in 2016 reaching to 7303 in 2019. 
```{python}
path_data = os.path.join(base_path,
 "pos2017.csv")
df_2017 = pd.read_csv(path_data)
 
path_data = os.path.join(base_path,
 "pos2018.csv")
df_2018 = pd.read_csv(path_data, encoding_errors = "ignore")

path_data = os.path.join(base_path,
 "pos2019.csv")
df_2019 = pd.read_csv(path_data, encoding_errors = "ignore")
```


```{python}
df_2017 = df_2017[
    (df_2017["PRVDR_CTGRY_SBTYP_CD"] == 1) & 
    (df_2017["PRVDR_CTGRY_CD"] == 1)]

df_2018 = df_2018[
    (df_2018["PRVDR_CTGRY_SBTYP_CD"] == 1) & 
    (df_2018["PRVDR_CTGRY_CD"] == 1)]

df_2019 = df_2019[
    (df_2019["PRVDR_CTGRY_SBTYP_CD"] == 1) & 
    (df_2019["PRVDR_CTGRY_CD"] == 1)]

# Add year column for each df for plot:
df_2016["year"] = 2016
df_2017["year"] = 2017
df_2018["year"] = 2018
df_2019["year"] = 2019
df_pos = pd.concat([df_2016,df_2017,df_2018,df_2019],
 ignore_index = True)
```

```{python}
import altair as alt
import warnings 
warnings.filterwarnings('ignore')
alt.renderers.enable("png")
alt.data_transformers.disable_max_rows()
bars = alt.Chart(df_pos).mark_bar().encode(
    alt.X("year:N", title = "Year"),
    alt.Y("count()",title = None, axis = None)
).properties(
    title = "Number of Short-term Hospitals per Year",
    width = 350, height = 200)
text = bars.mark_text(
    align="center",
    baseline="bottom",
    dy=-5  
).encode(
    text=alt.Text("count():Q")  
)

bars + text
```

4. 
    a. Upon aggregating the data based on the CMS certificate of each hospital, it seems that the numbers are identical to what previously found in (3)

```{python}
bars = alt.Chart(df_pos).transform_aggregate(
    unique_hospitals="count(PRVDR_NUM)",groupby=["year"]
    ).mark_bar().encode(
        x=alt.X("year:N", title="Year"),
    y=alt.Y("unique_hospitals:Q", axis = None, title = None)
).properties(
    title="Count of Unique Hospitals per Year",
    width=350,
    height=200
)
text = bars.mark_text(
    align="center",
    baseline="bottom",
    dy=-5  
).encode(
    text=alt.Text("unique_hospitals:Q")  
)

bars + text
```

    b. Upon comparing the two plots, the number of hospitals per year is identical to the number of unique hospitals based on the CMS certificate, this tells us that the data is strucutured in a way that prevents duplicate entries of hospitals per year, and the records are well strucutured to represent the number of hospitals accurately.

  

## Identify hospital closures in POS file (15 pts) (*)

1. There are `69` hospitals that fit the definition of suspected closure. A sorted list based on their names for the first 10 hospitals is shown below.
```{python}
suspected_hospitals = []

for year in range(2016, 2020):
    for hospital in df_pos["FAC_NAME"].unique():
        df_hospital_year = df_pos[(df_pos["FAC_NAME"] == hospital) & 
        (df_pos["year"] == year)]

        for i in range(len(df_hospital_year) - 1):
            if (df_hospital_year.iloc[i]["PGM_TRMNTN_CD"] == 0).any():
                if (df_hospital_year.iloc[i+1]["PGM_TRMNTN_CD"] != 0 |
                    pd.isna(df_hospital_year.iloc[i + 1]["PGM_TRMNTN_CD"])).any():
                    suspected_hospitals.append(hospital)
                    break
```
```{python}
df_suspected = df_pos[df_pos["FAC_NAME"].isin(suspected_hospitals)]
```

```{python}
df_suspected = df_suspected.groupby("FAC_NAME")[["ZIP_CD","PGM_TRMNTN_CD",
"PRVDR_NUM","year"]].apply(pd.DataFrame).reset_index()
```

```{python}
df_suspected_grouped = df_suspected.groupby("FAC_NAME").agg(
    ZIP_CD = ("ZIP_CD", "first"),
    Closure_year = ("year", "first")
).reset_index()
```

```{python}
# Number of suspected hospitals based on zip code
len(df_suspected_grouped)
```

2. Facility names of suspected closure hospitals sorted by name for the first 10 results:

```{python}
df_suspected_grouped.sort_values(by = "FAC_NAME").head(10)
```

3. 
    a. There are `49` hospitals that fit this definition of potentially
        being a merger/acquisition

```{python}
# Filter df_pos to include only active hospitals
suspected_merger = df_pos[df_pos["PGM_TRMNTN_CD"] != 0]

# Aggregate by year and zip codes
Active_hospitals = df_pos[df_pos["PGM_TRMNTN_CD"] == 0].groupby(
    ["year","ZIP_CD"])["PGM_TRMNTN_CD"].size().reset_index(name = "Active_count")

# Merging both datasets to compare counts
comparison_1 = suspected_merger.merge(
    Active_hospitals,left_on = "ZIP_CD", right_on = "ZIP_CD",
    suffixes =("_closure", "_active"))
# Creating a column for next year counts for comparison 
comparison_1["next_year"] = comparison_1["year_closure"].astype(int) + 1

# Getting active counts for next year
next_year_counts = Active_hospitals.rename(columns={"year": "next_year", "active_count": "next_year_active_count"})
# Comparing for next year
comparison_2 = comparison_1.merge(
    next_year_counts, left_on=["ZIP_CD", "next_year"], 
    right_on=["ZIP_CD", "next_year"], how="left")

# Sorting to include only non decreasing zips
non_decreasing = (
    comparison_2[comparison_2["Active_count_y"] >= comparison_2["Active_count_x"]])
```

```{python}
suspected_zips = set(df_pos["ZIP_CD"].unique()) - set(
    non_decreasing["ZIP_CD"].unique())

# Compare suspected zips with df_suspected group from 2.1
potential_merger = df_suspected_grouped[
    df_suspected_grouped["ZIP_CD"].isin(suspected_zips)]
len(potential_merger)
```

    b. Upon correcting we have 10480 hospitals left in total and 20 closed hospitals

```{python}
len(non_decreasing)
```

```{python}
corrected_closure = df_suspected_grouped[-df_suspected_grouped[
    "ZIP_CD"].isin(suspected_zips)]
len(corrected_closure)
```

    c.

```{python}
corrected_closure.sort_values(by = "FAC_NAME").head(10)
```

## Download Census zip code shapefile (10 pt) 

1. 
    a. There are five files zipped in this folder as the following:

    `dbf`: stores attribute information of spatial features, size 6.4 MB.
    `shp`: shape file which has has feature geometrics, size 837.5 MB.
    `prj`: describes the Coordinate Reference System (CRS), size 165 bytes.
    `shx`: contains positional index in .shp, size 265 KB.
    `xml`: metadata about the dataset, size 16 KB.

    b.     `dbf`:size 6.4 MB.
    `shp`: size 837.5 MB.
    `prj`: size 165 bytes.
    `shx`: size 265 KB.
    `xml`: size 16 KB.
    
2. 
```{python}
import geopandas as gpd 
```

```{python}
filepath = "c:/Users/danie/Documents/GitHub/problem-set-4-nasser-daniel"

path_shp = os.path.join(filepath,
 "gz_2010_us_860_00_500k.shp")

df_zip = gpd.read_file(path_shp)
```

    # Correct approach:
```{python}
# create a cleaned df for 2016
cleaned_pos = df_pos[-df_pos["FAC_NAME"].isin(df_suspected["FAC_NAME"])]
cleaned_pos_2016 = cleaned_pos[cleaned_pos["year"] == 2016]
```

```{python}
# create a list of filtered zips to compare with Texas zips in shp file
#hospital_zips_2016 = [str(int(zip_code)) for zip_code in cleaned_pos_2016["ZIP_CD"]]
hospital_zips_2016 = cleaned_pos_2016["ZIP_CD"].astype(int).astype(str).tolist()
filtered_zips_2016 = [zip_code for zip_code in hospital_zips_2016 if zip_code.startswith(("75", "76", "77", "78", "79"))]

texas_zip_codes = df_zip["ZCTA5"].isin(filtered_zips_2016)
texas_zip_codes = df_zip[texas_zip_codes]
texas_zip_codes["Hospital_zips"] = texas_zip_codes["ZCTA5"]

import matplotlib.pyplot as plt
texas_zip_codes.plot(column = "Hospital_zips", linewidth=0.2)
plt.axis("off")
plt.show()
```


```{python}
# Number of Hospitals in Texas:
texas_zips = texas_zip_codes["ZCTA5"].astype(int).tolist()
cleaned_pos_2016["ZIP_CD"] = cleaned_pos_2016["ZIP_CD"].astype(int)
len(cleaned_pos_2016[cleaned_pos_2016["ZIP_CD"].isin(texas_zips)])
```


## Calculate zip code’s distance to the nearest hospital (20 pts) (*)

1. 

```{python}
filepath = "c:/Users/danie/Documents/GitHub/problem-set-4-nasser-daniel"

# entire united states
path_shp = os.path.join(filepath,
 "gz_2010_us_860_00_500k.shp")
df_geo = gpd.read_file(path_shp)

```

```{python}
zips_all_centroids = df_geo.centroid
zips_all_centroids.shape
```

The resulting dimensions of this dataframe are (33120, 0)

2. 

```{python}
# just texas
zips_texas_centroids = texas_zip_codes["geometry"].centroid
zips_texas_centroids.shape
```

there are 482 unique zipcodes texas

```{python}
# texas and surrounding area
texas_range = [i for i in range(75, 80)]
missouri_range = [i for i in (range(716, 730))]
oklahoma_range = [i for i in (range(73, 75))]
new_mexico_range = [i for i in (range(870, 885))]
louisiana_range = [i for i in (range(700, 716))]

texas_and_surrounding_states_zips = []
texas_and_surrounding_states_zips.append(texas_range)
texas_and_surrounding_states_zips.append(missouri_range)
texas_and_surrounding_states_zips.append(oklahoma_range)
texas_and_surrounding_states_zips.append(new_mexico_range)
texas_and_surrounding_states_zips.append(louisiana_range)

#unpacking the list of lists
texas_and_surrounding_states_zips_list = pd.Series([zip for sublist in texas_and_surrounding_states_zips for zip in sublist]).astype(str)

texas_and_surrounding_states_zips_tuple = []
for i in range(len(texas_and_surrounding_states_zips_list)):
    value = texas_and_surrounding_states_zips_list[i]
    texas_and_surrounding_states_zips_tuple.append(value)

texas_and_surrounding_states_zips_tuple = tuple(texas_and_surrounding_states_zips_tuple)

#identifying texas and surrounding zipcodes from geodataframe
hospital_zips_2016 = cleaned_pos_2016["ZIP_CD"].astype(int).astype(str).tolist()
filtered_zips_2016 = [zip_code for zip_code in hospital_zips_2016 if
 zip_code.startswith(texas_and_surrounding_states_zips_tuple)]

texas_and_surrounding_zip_codes = df_zip["ZCTA5"].isin(filtered_zips_2016)
texas_and_surrounding_zip_codes = df_zip[texas_and_surrounding_zip_codes]
texas_and_surrounding_zip_codes["Hospital_zips"] = texas_and_surrounding_zip_codes["ZCTA5"]

zips_texas_borderstates_centroids = texas_and_surrounding_zip_codes["geometry"].centroid
zips_texas_borderstates_centroids.plot()
zips_texas_borderstates_centroids.shape

```

There are 868 unique zip codes in the texas and surrounding states centroid variable. 

3. 

```{python}
active_hospitals_2016 = df_pos[df_pos["PGM_TRMNTN_CD"] == 0]["ZIP_CD"].astype(int).unique()
#take these zip codes and find them in the geo dataset
min_one_hospital = df_geo["ZCTA5"].astype(int).isin(active_hospitals_2016)
min_one_hospital = df_geo[min_one_hospital]
min_one_hospital["ZIP_CD"] = min_one_hospital["ZCTA5"]

zips_withhospital_centroids = gpd.sjoin(texas_and_surrounding_zip_codes, min_one_hospital, how = "inner").centroid

zips_withhospital_centroids.plot()
```

4. 

    a.

```{python}

distance_gdf = gpd.GeoDataFrame()
distance_gdf['nearest_hospital_distance'] = zips_withhospital_centroids.geometry.apply(
    lambda x: zips_texas_borderstates_centroids.head().distance(x).min())

distance_gdf
```

This shortened version took 16 seconds. The longer version could take multiple minutes.

```{python}

distance_gdf_full = gpd.GeoDataFrame()
distance_gdf_full['nearest_hospital_distance'] = zips_withhospital_centroids.geometry.apply(
    lambda x: zips_texas_borderstates_centroids.distance(x).min())

distance_gdf_full
```

    b. I fooled myself. This was much quicker than I expected. It seems that every zipcode has at least one active hospital in it in 2016. Therefore the computer did not have many intensive calculations to complete, relative to the first attempt at this.
5. 
    a.
    b.
    c.
    
## Effects of closures on access in Texas (15 pts)

1. 
2. 
3. 
4. 

## Reflecting on the exercise (10 pts) 
